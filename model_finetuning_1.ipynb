{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXH1iCxWARpb",
        "outputId": "4070c7c5-ea04-4b31-9947-571d4f07b53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0jWHtxwtRFC",
        "outputId": "65d0d380-2e8b-4011-9215-d6b3feb3c573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnTe6k9etQdH",
        "outputId": "8eadd43d-df17-4a2d-9384-9c52eae8ad69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n",
            "Average Loss: 1.0632626414299011\n",
            "Epoch 2/5\n",
            "----------\n",
            "Average Loss: 1.3177355527877808\n",
            "Epoch 3/5\n",
            "----------\n",
            "Average Loss: 0.9382308721542358\n",
            "Epoch 4/5\n",
            "----------\n",
            "Average Loss: 0.9894682466983795\n",
            "Epoch 5/5\n",
            "----------\n",
            "Average Loss: 1.1048793196678162\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def read_csv(filename, column_name):\n",
        "    df = pd.read_csv(filename)\n",
        "    data = df[column_name]\n",
        "    return data\n",
        "\n",
        "\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, queries, labels):\n",
        "        self.queries = queries\n",
        "        self.labels = labels\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        query = self.queries[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            query,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "dataset = SQLDataset(queries, labels)\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    print(f'Average Loss: {average_loss}')\n",
        "\n",
        "\n",
        "model.save_pretrained('bert-sql-model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "def read_csv(filename, column_name):\n",
        "    df = pd.read_csv(filename)\n",
        "    data = df[column_name]\n",
        "    return data\n",
        "\n",
        "\n",
        "class TextQueryDataset(Dataset):\n",
        "    def __init__(self, texts, queries, labels):\n",
        "        self.texts = texts\n",
        "        self.queries = queries\n",
        "        self.labels = labels\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        query = self.queries[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            query,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "texts = read_csv(\"dataset_text.csv\", 'text')\n",
        "queries = read_csv(\"dataset_cypher_skimmed.csv\", 'cypher')\n",
        "labels = [i for i in range(len(texts))]\n",
        "\n",
        "t = texts[:32]\n",
        "q = queries[:32]\n",
        "l = labels[:32]\n",
        "dataset_train = TextQueryDataset(t, q, l)\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(dataset_train))\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "# dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def train():\n",
        "  avg_loss = []\n",
        "\n",
        "  epochs = 5\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      print(f'Epoch {epoch + 1}/{epochs}')\n",
        "      print('-' * 10)\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      for batch in dataloader_train:\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          labels = batch['labels'].to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          labels=labels)\n",
        "\n",
        "          loss = outputs.loss\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "      average_loss = total_loss / len(dataloader_train)\n",
        "      avg_loss.append(average_loss)\n",
        "      print(f'Average Loss: {average_loss}')\n",
        "\n",
        "  model.save_pretrained('bert-sql-model')\n",
        "\n",
        "  return avg_loss\n",
        "\n",
        "store_loss = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6wYX61Dtc-U",
        "outputId": "3a3033d9-010c-4ef1-8c9e-9214d93f2b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n",
            "Average Loss: 3.548569083213806\n",
            "Epoch 2/5\n",
            "----------\n",
            "Average Loss: 3.466788187623024\n",
            "Epoch 3/5\n",
            "----------\n",
            "Average Loss: 3.39273239672184\n",
            "Epoch 4/5\n",
            "----------\n",
            "Average Loss: 3.427923157811165\n",
            "Epoch 5/5\n",
            "----------\n",
            "Average Loss: 3.299599841237068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.plot(store_loss, label = \"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "wiw18yP6tNmZ",
        "outputId": "f99a3a5b-66e3-421a-d41c-ebf273017e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-sql-model')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "text = \"What are some recipes that can be served as both dinner and dessert?\"\n",
        "sql_query = \"MATCH (r:Recipe) WHERE 'dinner' IN r.meal_types AND 'dessert' IN r.serving_suggestions RETURN r.name\"\n",
        "\n",
        "\n",
        "encoding = tokenizer.encode_plus(\n",
        "    text,\n",
        "    sql_query,\n",
        "    add_special_tokens=True,\n",
        "    padding='max_length',\n",
        "    max_length=128,\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "input_ids = encoding['input_ids'].flatten()\n",
        "attention_mask = encoding['attention_mask'].flatten()\n",
        "\n",
        "\n",
        "outputs = model(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
        "\n",
        "\n",
        "predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "print(\"Predicted Label:\", predicted_label)\n",
        "\n",
        "print(texts[predicted_label])\n",
        "\n",
        "print(queries[predicted_label])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arpYWd2q9eyz",
        "outputId": "08119dcb-bd4a-4251-9094-215543c89224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 9\n",
            "Could you suggest some Moroccan recipes that are both healthy and low in sodium?\n",
            "MATCH (r:Recipe) WHERE r.cuisine = 'Moroccan' AND r.healthiness = 'healthy' AND r.sodiumLevel = 'low' RETURN r.name r.description r2.name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "hypothesis = queries[predicted_label].split(\" \")\n",
        "print(hypothesis)\n",
        "\n",
        "reference = sql_query.split(\" \")\n",
        "print(reference)\n",
        "\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "print(BLEUscore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta0EtDf2sJa4",
        "outputId": "7dc06285-8944-496f-84f9-79bce167d65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MATCH', '(r:Recipe)', 'WHERE', 'r.cuisine', '=', \"'Moroccan'\", 'AND', 'r.healthiness', '=', \"'healthy'\", 'AND', 'r.sodiumLevel', '=', \"'low'\", 'RETURN', 'r.name', 'r.description', 'r2.name']\n",
            "['MATCH', '(r:Recipe)', 'WHERE', \"'dinner'\", 'IN', 'r.meal_types', 'AND', \"'dessert'\", 'IN', 'r.serving_suggestions', 'RETURN', 'r.name']\n",
            "3.0074186032008e-78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class TextSqlQueryDataset(Dataset):\n",
        "    def __init__(self, texts, queries, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.queries = queries\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        query = self.queries[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            query,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].flatten()\n",
        "        attention_mask = encoding['attention_mask'].flatten()\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'target_ids': input_ids,\n",
        "            'target_attention_mask': attention_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "texts = ['The weather is nice today.',\n",
        "         'I need a recipe for spaghetti.',\n",
        "         'What are the top tourist attractions in Paris?']\n",
        "queries = ['SELECT * FROM weather WHERE date = TODAY;',\n",
        "           'SELECT * FROM recipes WHERE dish = \"spaghetti\";',\n",
        "           'SELECT * FROM attractions WHERE city = \"Paris\";']\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "dataset = TextSqlQueryDataset(texts, queries, tokenizer)\n",
        "\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        target_ids = batch['target_ids'].to(device)\n",
        "        target_attention_mask = batch['target_attention_mask'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=target_ids,\n",
        "            decoder_attention_mask=target_attention_mask,\n",
        "            labels=target_ids\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    print(f'Average Loss: {average_loss}')\n",
        "\n",
        "\n",
        "model.save_pretrained('t5-text-sql-query-generation-model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "sH5hSO43ARmV",
        "outputId": "83a3baaf-55ee-47a1-8d29-a65c2de9bf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-919f52a3888a>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m            'SELECT * FROM attractions WHERE city = \"Paris\";']\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextSqlQueryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers==0.10.3\n",
        "!pip install transformers==4.5.0\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bqAznF5oARsS",
        "outputId": "955ecdf5-33c2-40ab-da21-0e4f1425b404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers==0.10.3\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.5.0\n",
            "  Using cached transformers-4.5.0-py3-none-any.whl (2.1 MB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.5.0) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.5.0) (4.65.0)\n",
            "Collecting sacremoses\n",
            "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==4.5.0) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.5.0) (2.27.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.5.0) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.5.0) (2022.10.31)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.5.0) (2.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.5.0) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.5.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.5.0) (8.1.3)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-76b518e6bb5a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tokenizers==0.10.3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers==4.5.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-sql-model')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "text = \"Can you suggest a good Italian restaurant in New York?\"\n",
        "sql_query = \"SELECT name, address FROM restaurants WHERE cuisine = 'Italian' AND city = 'New York'\"\n",
        "\n",
        "\n",
        "encoding = tokenizer.encode_plus(\n",
        "    text,\n",
        "    sql_query,\n",
        "    add_special_tokens=True,\n",
        "    padding='max_length',\n",
        "    max_length=128,\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids = encoding['input_ids'].flatten()\n",
        "attention_mask = encoding['attention_mask'].flatten()\n",
        "\n",
        "\n",
        "outputs = model(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
        "\n",
        "\n",
        "predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "print(\"Predicted Label:\", predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfqwsLas_gYu",
        "outputId": "4e903d68-8e93-455d-ff68-66848442ce17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class TextQueryDataset(Dataset):\n",
        "    def __init__(self, texts, queries, labels):\n",
        "        self.texts = texts\n",
        "        self.queries = queries\n",
        "        self.labels = labels\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        query = self.queries[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            query,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "texts = ['The weather is nice today.',\n",
        "         'I need a recipe for spaghetti.',\n",
        "         'What are the top tourist attractions in Paris?']\n",
        "queries = ['What is the weather like?',\n",
        "           'How do I make spaghetti?',\n",
        "           'List popular tourist spots in Paris.']\n",
        "labels = [0, 1, 2]\n",
        "\n",
        "dataset = TextQueryDataset(texts, queries, labels)\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    print(f'Average Loss: {average_loss}')\n",
        "\n",
        "\n",
        "model.save_pretrained('bert-text-query-model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jppEa8eIzKuB",
        "outputId": "85006b11-457a-44fe-ee7a-9d4387161007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n",
            "Average Loss: 0.9184134304523468\n",
            "Epoch 2/5\n",
            "----------\n",
            "Average Loss: 1.0645493865013123\n",
            "Epoch 3/5\n",
            "----------\n",
            "Average Loss: 0.9430181384086609\n",
            "Epoch 4/5\n",
            "----------\n",
            "Average Loss: 0.7694268822669983\n",
            "Epoch 5/5\n",
            "----------\n",
            "Average Loss: 0.767183244228363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach - 2"
      ],
      "metadata": {
        "id": "5ziHqa4Eq1VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelWithLMHead.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "vIrK_qfdJuTB",
        "outputId": "823ab014-0a94-4f2a-fb5f-b241e7bd44db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = [] \n",
        "output_sequences = []\n",
        "\n",
        "\n",
        "for i in range(len(texts)):\n",
        "  input_sequence = tokenizer.encode(texts[i], add_special_tokens=False)\n",
        "  output_sequence = tokenizer.encode(queries[i], add_special_tokens=False)\n",
        "\n",
        "  input_sequence.append(tokenizer.sep_token_id)\n",
        "  input_sequence.extend(output_sequence)\n",
        "\n",
        "  input_sequences.append(input_sequence)\n",
        "  output_sequences.append(output_sequence)\n"
      ],
      "metadata": {
        "id": "7BbH37NXJxQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=input_sequences,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "D0Z2eSzsLQbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "03e2a051-e701-48d1-fa54-334236a18e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-dc33188f0e64>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m         )\n\u001b[0;32m-> 1662\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1663\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2699\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2731\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2732\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1595\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1598\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (3) to match target batch_size (63)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What are some healthy Moroccan recipes that are low in sodium?\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "output_ids = model.generate(input_ids, max_length=100, do_sample=True)\n",
        "\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "zGbgUa8kLTLp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "58e01968-63ea-48a7-9aad-4f9d6192b61e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-a8eb749a9841>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moutput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0;31m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0;31m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgenerate_compatible_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 \u001b[0mexception_message\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" Please use one of the following classes instead: {generate_compatible_classes}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The current model class (BertForSequenceClassification) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BertLMHeadModel'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entities = {\"person\": [\"John Doe\", \"Jane Smith\"], \"city\": [\"New York\"]}\n",
        "\n",
        "\n",
        "query = \"MATCH (p:person)-[:LIVES_IN]->(c:city) WHERE \"\n",
        "\n",
        "for entity_type, entity_values in entities.items():\n",
        "    if entity_type == \"person\":\n",
        "        query += \"p.name IN \" + str(entity_values) + \" AND \"\n",
        "    elif entity_type == \"city\":\n",
        "        query += \"c.name IN \" + str(entity_values) + \" AND \"\n",
        "\n",
        "\n",
        "query = query[:-4] + \"RETURN p, c\"\n",
        "\n",
        "\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxsX5WXcLd_j",
        "outputId": "36881329-db5d-4bc5-e362-b06db89acd0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MATCH (p:person)-[:LIVES_IN]->(c:city) WHERE p.name IN ['John Doe', 'Jane Smith'] AND c.name IN ['New York'] RETURN p, c\n"
          ]
        }
      ]
    }
  ]
}